---
layout: mechanics
title: Теория Массового Обслуживания
order: 100
---
<div class="chapter_quote"><p>
Удовольствие от инжиниринга - это найти прямую линию на графике логарифма в квадрате.
<br/>
—Томас Кёниг
</p></div>


Теория массового обслуживания позволяет разобраться почему традиционная разработка чрезмерно медленная - и что с этим делать. В крупномасштабной разработке распространено, что "одна" фича (до разделения) может быть ошеломляюще гигантской. В таких областях, особенно хорошо заметно, что большие партии и длинные очереди действительно существуют и какие проблемы они вызывают. Сложно решить проблему, о существовании которой вы не знаете. И теория массового обслуживания указывает на некоторые способы для улучшений. Этот инструмент мышления особенно релевантен для крупных масштабов, поскольку большие, меняющиеся партии работы --- столь распространенные в традиционной модели --- имеют *нелинейное воздействие* на время цикла --- они *действительно* могут всё испортить,

<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-1.jpg" alt="queueing-1.jpg">
  <figcaption>Эта система имеет нелинейную динамику!</figcaption>
</figure>

Интересное несоответствие: Теория массового обслуживания --- математический анализ того как *вещи* двигаются сквозь систему с очередями --- была разработана, чтобы понимать и улучшать пропускную способность в телекоммуникационных системах --- системах с большой вариативностью и хаотичностью близкой к продуктовой разработке. Как следствие, телеком инженеры понимают основные идеи. Но по-прежнему, люди, занимающиеся развитием телеком инфраструктуры (телеком - это область больших продуктов) редко замечают, что *это применимо к их системе*, чтобы сократить среднее время цикла в их производственном процессе.

Люди из Тойота изучают статистическую вариацию и влияние теории массового обслуживания; это отражается в *выравнивающем* принципе бережливого подхода, который уменьшает вариативность и в целом в фокусе бережливого подхода на уменьшении размера партии и времени цикла прохождения *потока*.

Прежде чем погружаться в тему, отметим, что бережливый подход иногда описывается как концентрация на *меньших размерах партии*, *более коротких очередях*, и *сокращении времени цикла*. Быстрая поставка ценности. Бережливый подход существенно больше этого --- его столпы - *уважение к людям* и *непрерывное улучшение* которые опираются на основы в виде *руководителей-учителей в бережливом мышлении*. **Управление очередями (ограничение НЗР, ...) эффективная практика, но это лишь инструмент, далекий от глубокой сущности бережливого мышления.**

Как станет очевидно, LeSS поддерживает управленческие последствия теории массового обслуживания.

##  Миф Закона Литтла

Прежде чем дальше погружаться в теорию массового обслуживания, несколько слов о широко распространённом мифе, относящемся к теории массового обслуживания, распространённом в сообществах гибкой разработки и бережливого подхода (и даже в нескольких подходах к масштабированию), который следует незамедлительно прояснить. [Закон Литтла](http://web.mit.edu/sgraves/www/papers/Little's%20Law-Published.pdf) доказывает, что *уменьшение уровня НЗР (WIP) уменьшает среднее время цикла*. О, если бы это было так! К сожалению, доказательство Литтла зависит от ряда условий/предположений, которые должны соблюдаться для того, чтобы динамика оставалась положительной. И, к сожалению, эти условия никак не гарантируются или даже не являются как-правило-истинными в областях с высокой степенью изменчивости, таких как разработка программного обеспечения. Наивность продвижения или обоснования Закона Литтла в разработке программного обеспечения хорошо разобрана в анализе Даниэля Ваканти (Daniel Vacanti), названном [Ошибка Литтла] ([Little's Flaw]) (http://vimeo.com/52683659)


**Уменьшение уровня НЗР - достойная цель и значимо в LeSS. НЗР - это одна из потерь в бережливом мышлении, потому что это, помимо прочих проблем, задерживает возврат инвестиций, скрывает дефекты, уменьшает прозрачность. И уменьшение уровня НЗР выявляет недостатки. Но, к сожалению, в разработке программного обеспечения нет гарантии, что уменьшение НЗР автоматически уменьшит среднее время цикла.**
{: .box_top_bottom }

## Конкурировать за счёт Укорачивания Времени Цикла

Бережливая организация разрабатывающая продукты сосредоточена на *пропускной способности при поставке ценности в стабильных циклах за кратчайшее возможное время*, она сосредоточена прежде всего на *пропускной способности*, а не на занятости людей. Люди из Тойота, прародители бережливого мышления, являются мастерами во всё большем и большем ускорении (укорачивании) времени цикла **без перегрузки людей**.

Какие есть некоторые процессные циклы или что есть время цикла в продуктовой разработке?

* "от идеи до прибыли" за один релиз
* "от идеи до готовности" в одной фиче
* время до потенциальной готовности к поставке; как часто вы можете поставлять?
* время компиляции (для всего программного обеспечения)
* время от "готово к пилоту" до поставки
* время развертывания для тестирования (на интегрированном оборудовании)
* время на анализ и проектирование
{: .two_columns .box_top_bottom}

Ключевые показатели эффективности (КПЭ) в бережливом подходе *не* сосредоточены на утилизации или занятости рабочих, выполняющих вышеописанные процессы. Скорее, *КПЭ бережливого подхода фокусируются больше на пропускной способности и времени цикла*.

Тем не менее, предупреждение: Измерение обычно приводит к дисфункции или к попыткам 'обыграть' систему путем локальной оптимизации для того, чтобы создать видимость, как будто достигаются хорошие показатели. [[Austin96]](http://www.amazon.com/Measuring-Managing-Performance-Organizations-Dorset-ebook/dp/B00DY3KQX6/ref=sr_1_1?ie=UTF8&qid=1413596674&sr=8-1&keywords=measuring+and+managing+performance+in+organizations). Это особенно справедливо для 'низкоуровневых' процессных циклов. Показатели времени цикла более высокого уровня, такие как время цикла до потенциальной готовности к поставке и "от заказа до прибыли" или "от заказа до поставки" (наиболее существенные показатели времени цикла), являются более актуальными.

Что бы могла значить для вас возможность поставлять в *два или четыре раза быстрее* в *устойчивом темпе* без перегрузки людей? И с другой стороны, какова стоимость задержки?

Рассмотрите преимущества быстрой доставки с точки зрения жизненного цикла, полученных возможностей, реакции на конкуренцию и инноваций. Для большинства компаний---не для всех---это будет *выдающееся* преимущество.

**В два раза быстрее - это не в два раза дешевле**{: style="color: #1997C0"}---Когда люди слышат "в два раза быстрее" они возможно думают: "В два раза больше продуктов, функциональностей или релизов---вдвое эффективнее." Но здесь могут скрываться **операционные издержки**, накладные расходы в каждом цикле. Более частая поставка может увеличить расходы на тестирование или установку---или нет, как будет видно далее.

**В два раза быстрее - это не в два раза дороже**{: style="color: #1997C0"}---Не торопитесь убирать свои таблицы для анализа операционных расходов. Существует тонкая связь между временем цикла, операционными расходами и эффективностью, как вскоре будет прояснено---секрет, стоящий за впечатляющей эффективностью Toyota и других предприятий, использующих бережливое мышление ...

**Управление очередями**{: style="color: #1997C0"}---существует множество стратегий, нацеленных на уменьшение времени цикла; приверженцы и бережливого производства и практики гибкой разработки предлагают нам рог изобилия различных практичных средств. Один из таких инструментов является темой настоящего раздела---управление очередями.

##Управление Очередями для Уменьшения Времени Цикла

"*Очереди существуют только в производстве, так что теория массового обслуживания и управление очередями неприменимы к продуктовой разработке*." Это распространенное заблуждение. Как упоминалось ранее, теория массового обслуживания зародилась не в производстве, а в исследованиях операций для повышения пропускной способности в телекоммуникационных системах с высокой вариативностью. Тем не менее, многие группы разработки---особенно те, которые внедряют практики гибкой разработки или бережливого мышления---приняли управление очередью, основанное на понимании теории массового обслуживания как для *разработки продуктов*, так и для *управления портфелем*. Авторы одного исследования в МТИ (MIT, Массачусетский технологический институт) и Стэнфордском университете, пришли к следующему выводу:

> Бизнес подразделения, которые приняли этот подход [управление очередями для продуктового портфеля и управления продуктами] уменьшили среднее время разработки на 30-50% [[AMNS96]](http://www-bcf.usc.edu/~padler/research/HBR_prod_dev_proc.pdf)

### Очереди в Разработке Продукта и Управлении Портфелем

Примеры очередей в разработке и управлении портфелем?

* продукты или проекты в портфеле
* новые функциональности для одного продукта
* подробные спецификации требований, ожидающие проектирования
* проектные документы, ожидающие разработки
* код ожидающий тестирования
* код одного разработчика ожидающий интеграции с другими разработчиками
* большие компоненты, ожидающие интеграции
* большие компоненты и системы ожидающие тестирования
{: .two_columns .box_top_bottom}



В традиционной последовательной разработке существует множество очередей с частично сделанной работой, известные как не-завершенная-работа **очереди НЗР **; например, документы со спецификацией, ожидающие начала программирования или код, ожидающий начала тестирования.

В добавлении к *очередям НЗР* , существуют **ограниченные-ресурсы** или **очереди разделяемых-ресурсов**, такие как бэклог запросов на использование дорогостоящей тестовой лаборатории или оборудования для тестирования.

### Очереди Являются Проблемой

Во-первых, если не существует очередей---и нет многозадачности, которая искуственно создаёт видимость, что очередь устранена---тогда система двигается в направлении потока, принцип бережливого подхода и стремления к совершенству в том, что ценность поставляется без каких-либо задержек. Каждая очередь создаёт задержку, которая является препятствием для потока поставки ценности. Если говорить более конкретно, почему очереди являются проблемой?

#### Очереди НЗР

Очереди НЗР в разработке продуктов  редко рассматриваются как таковые по нескольким причинам; возможно главная среди них в том, что они как правило *незаметны* ---набор битов на диске компьютера. Но они действительно существуют---и что более важно, они создают проблемы. Почему?

Очереди НЗР (как и большинство очередей) *увеличивают среднее время цикла* и уменьшают поставку ценности и тем самым могут снизить общую прибыль.

В бережливом мышлении, очереди НЗР классифицируются как *потери* ---и, следовательно, должны быть устранены или сокращены---потому, что:

* Очереди НЗР имеют вышеупомянутое воздействие на время цикла.
* Очереди НЗР являются *частично-готовыми запасами* (спецификаций, кода, документации, ...) с вложением времени и денег, для которых ещё не произошло возврата инвестиций.
* Так же как и все запасы, очереди НЗР скрывают ошибки---и способствуют их распространению---поскольку груда запасов  ещё не была использована или проверена на следующем шаге процесса, чтобы проявить скрытые проблемы; например, большое количество ещё не-интегрированного кода.
* Одна история: Мы видели продуктовую группу, использующую традиционные подходы к разработке,  которая потратила около года, работая над функциональностью “завершение сделки”. Затем управление продуктом решило удалить её, потому что она поставила под угрозу весь релиз и рыночные условия изменились. Перепланирование заняло много недель. В целом, очереди НЗР *влияют на стоимость и способность реагировать на изменения* (удаления и добавления) потому, что: (1) время и деньги были потрачены на незавершенную удалённую работу которая не будет в конечном итоге реализована, или  (2) НЗР, связанная с удалёнными элементами, может быть тесно переплетена с другой функциональностью, или (3) новая функциональность, ожидающая добавления, может испытать отложенный запуск из-за высокого уровня текущей НЗР.
<br>

Как будет рассмотрено далее, существует тонкий, но потенциально очень мощный побочный эффект улучшения-систем, который может возникнуть в процессе устранения очередей НЗР.

#### Очереди Разделяемых-Ресурсов

В противоположность очередям НЗР (которые на первый взгляд не кажутся тем, чем являются), очереди разделяемых-ресурсов чаще всего **видятся как очереди**---и видятся как проблема. Они действительно заметно и болезненно замедляют людей, задерживают обратную связь и растягивают время цикла. *“Нам нужно протестировать свой новый код на том целевом принтере в тестовой лаборатории. Когда он будет свободен?”*

#### План А: Ликвидация (а не управление) Очередями

В итоге суть такова, что (как правило) *очереди являются проблемой*. Исходя из этого, вы можете сделать поспешный вывод, что первая линия обороны против этой проблемы - *уменьшить объёмы партий и размер очереди* , потому, что таковы классические стратегии управления-очередью. Однако существует решение, позволяющее "разрубить [Гордиев Узел](https://en.wikipedia.org/wiki/Gordian_Knot)", которое следует рассмотреть первым...

В оставшейся части этой главы мы, несомненно, рассмотрим уменьшение времени цикла путём управления размерами партий-и очередей-. Но вся эта стратегия управления должна быть *Планом Б*. Скорее, начать следует с *Плана А*:

**План А в исправлении проблемы очереди заключается в полном искоренении этой очереди, навсегда, изменив саму систему: организационную систему, систему разработки, инструменты, процессы, практики, политики, и т.п.**
{: .box_top_bottom }

Превзойдите границы мышления и сократите время цикла, изменив саму *систему* так, чтобы очереди больше не существовало---устраняя узкие места (bottleneck) и другие воздействия, являющиеся причинами очередей. Эти ограничения и очереди, которые они порождают, могут быть как созданы так и уничтожены самой природой системы разработки и её инструментами.

Предположим существующая система основывается на последовательной разработке с рабочими или группами одной-специализации/функции. В такой системе будут существовать очереди НЗР: Группа аналитиков передаёт пакеты рабочих спецификаций группе разработчиков, которая передаёт пакеты рабочего кода группе тестирования, которая передаёт код группе внедрения и эксплуатации.  Мерами, лежащими в *пределах-границ-мышления*. по улучшению с помощью управления очередями, является уменьшение лимитов НЗР и размеров очередей. Но такой подход имеет дело всего лишь со вторичными симптомами существующей системы.

Существует более фундаментальная альтернатива, которая может радикально улучшить время цикла: Откажитесь от существующей системы, от её узких мест и от очередей НЗР, которые они порождают. Если вы внедрите **кросс-функциональные продуктовые команды, которые поставляют полностью готовую функциональность** (включая аналитику, программирование и тестирование), без передачи работы другим группам и которые применяют автоматизированную **разработку через приёмочное тестирование** и **автоматизированную непрерывную поставку**, то вышеупомянутые очереди НЗР *исчезнут* благодаря переходу от последовательной к параллельной разработке.

Этот подход отражает видение, основанное на исправлении-корневых-причин в LeSS: Измените организационную структуру для решения ключевых проблем.


#### Фальшивая Ликвидация Очереди

Предположим вы заняты работой над элементом А, а элементы Б, В, Г  и Д находятся в вашей очереди. Фальшивым сокращением очереди было бы работать над *всеми* этими элементами в примерно одно и то же время---высокий уровень многозадачности и утилизации. Многозадачность считается одной из потерь в бережливом подходе, потому что, как мы вскоре увидим, теория массового обслуживания показывает, что это способствует *увеличению* среднего времени цикла,  а не его уменьшению. И этот фальшивый подход фактически увеличивает уровень НЗР, во имя его уменьшения! Плохая идея.

Не увеличивайте многозадачность или уровень утилизации для создания *иллюзии* что очередь была уменьшена и система была улучшена; скорее, улучшайте саму систему, чтобы узкие места и другие воздействия, которые создают очереди, были устранены.

#### План Б : Управляйте Очередями Когда Вы Не Можете Ликвидировать Их

Распространённые очереди НЗР могут быть ликвидированы переходом к LeSS с кросс-функциональными продуктовыми командами, применением разработки через приёмочное тестирование (A-TDD) и непрерывной поставки. Изгнанные и исчезнувшие благодаря Плану А---изменение самой системы. Это идеальное решение, рекомендованное в LeSS.

Однако по-прежнему очереди могут оставаться и остаются, такие, как:

* очереди разделяемых-ресурсов, таких, как тестовая лаборатория
* очередь запросов на новую функциональность Бэклоге Продукта
* очереди НЗР потому, что: (1) План А пока ещё не осуществим (глубокие изменения в больших группах требуют времени), или (2) инструменты и техники, такие как переход от ручного к полностью автоматизированному тестированию, пока слабы и улучшаются медленно

По какой бы причине очереди не оставались---и в конце концов, Бэклог Продукта останется всегда---вы можете улучшить средне время цикла благодаря Плану Б, управляя очередями:

**План Б: Для очередей, которые не могут искоренены, улучшайте среднее время цикла путём уменьшения объёмов партий в очередях, уменьшения лимитов НЗР и размеров очередей и стараясь делать все партии более-менее одинакового размера.**
{: .box_top_bottom }

В LeSS, меньший объём партии означает меньший рабочий пакет элементов или функциональности для разработки в Спринте. Выровненные по размеру партии подразумевают, что каждая такая порция работ оценивается примерно в одинаковое количество затрат.

Как конкретно применить это в LeSS? Это будет раскрыто чуть позже, а сначала,  погрузимся в область теории массового обслуживания...











## Теория Массового Обслуживания

Это могло бы потребовать тяжелой работы или какого-то нового подхода, но не нужно много теории для того, чтобы "управлять очередями” путём их *ликвидации*. С другой стороны, когда они должны существовать ни смотря ни на что, полезно знать как иметь с ними дело с помощью инструмента мышления теории массового обслуживания.

### Формальная Модель для Оценки Процессов

Вы можете принять за чистую монету, что очереди с меньшими партиями-функциональностей, одинакового размера улучшают среднее время цикла. Или нет. В любом случае, полезно знать, что эта рекомендация не основывается на чьём-либо мнении, а опирается на формальную математическую модель, которая может быть подтверждена. Некоторые аспекты процесса разработки *действительно* возможно аргументировать с помощью формальной модели. Например:

* *Гипотеза* : Наиболее верный способ - последовательная (‘водопадная’ или V-модель) разработка с передачей больших объёмов партий между специализированными функциональными группами.
* *Гипотеза* : Наиболее правильно для людей или групп - иметь высокие показатели утилизации и многозадачности на многих проектах в одно и то же время.
<br>

Понимание теории массового обслуживания, свободной от чьих-либо мнений, поможет выявить действительно ли эти гипотезы помогают уменьшить среднее время цикла.

Данная тема относительно проста; пример сценария, охватывающего ключевые аспекты ...

### Качества Стохастической Системы с Очередями

<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-1.jpg" alt="queueing-1.jpg">
  <figcaption>Эта система имеет нелинейную динамику!</figcaption>
</figure>

Рассмотрим Лос-Анджелес или Бангалор в час пик. Пока каким-то чудом здесь нет аварий и все полосы открыты. Поток плотный и медленный, но движется. Затем за короткий промежуток времени, происходят аварии на трёх различных основных четырёхполосных шоссе (всего двенадцать полос) и три линии закрываются---только девять линий остаются по-прежнему открытыми. Буум! Прежде чем вы успеете сказать: “Почему я не купил вертолёт?” на большей части города происходит *сдвиг фазы* в состояние *пробка*. Когда последствия аварий окончательно расчищены (в последующем промежутке от тридцати до шестидесяти минут), проходит *вечность*, прежде чем расчистится огромная накопленная очередь. Наблюдения:

* **Нелинейность**{: style="color: #1997C0"}---Когда шоссе загружено от нуля до пятидесяти процентов, оно плавно плывёт---практически нет задержек или очередей. Но междц пятьюдесятью и ста процентами, замедление становится заметным, начинают выстраиваться очереди. Связь загрузки шоссе с размером очереди не линейное, нет плавного линейного увеличения от нуля.
* **Задержки и перегрузка не начинается с 99.99% загрузки**{: style="color: #1997C0"}---Это *не* тот случай, когда всё на шоссе двигается быстро и плавно, до тех пор пока не достигнута 100-процентная загрузка машин на дороге. Напротив, всё замедляется и появляются пробки задолго до того как достигнута максимальная загрузка.
* **Расчистка очереди занимает больше времени чем её создание**{: style="color: #1997C0"}---сорокапятиминутный затор в Лос-Анджелесе в час пик создаёт очереди, расчистка которых занимает более сорока пяти минут.
* **Стохастичность, не детерминированность**{: style="color: #1997C0"}---Существует вариация и беспорядочность в вероятности (это **стохастическая** система): прибытия машин, времени устранения заторов, скорости выхода автомобилей из пробок.

Это стоит прояснить, если вы хотите разобраться в поведении системы, поскольку, судя по всему, все мы, люди, *не* имеем интуитивного понимания стохастических и нелинейных качеств систем с очередями. Наоборот, внутренний инстинкт говорит, что они детерминированы и ведут себя линейно. Этот ошибочный “здравый смысл” приводит к *ошибочному мышлению* при анализе проблем и управлении разработкой продуктов. Эти наблюдения---как и ошибки мышления---применимы также и к очередям НЗР  в традиционной продуктовой разработке и практически ко всем другим очередям.

Одна распространённая ошибка мышления в разработке продуктов заключается в том, что очереди, задержки и люди, которые их обслуживают, ведут себя как на Рисунке 1---это заблуждение “*задержки начинаются только тогда, когда шоссе заполнено на 100 процентов.* ” На самом деле задержки  начинают возникать на шоссе задолго до того, как оно заполнится на 100 процентов. Возможно вы начнёте наблюдать задержки при 60-процентной загрузке---а значит, более долгое среднее время цикла.


<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-4.png" alt="queueing-4.png">
  <figcaption>Рис. 1. Распространённый миф или ошибка мышления по поводу очередей в системах с вариативностью.</figcaption>
</figure>


Из-за заблуждения “*задержки начинаются только тогда, когда шоссе заполнено на 100 процентов,* ” возникает неправильный фокус на попытках улучшить время цикла путём *увеличения* утилизации ресурсов---заставляя людей, разрабатывающих продукт, быть более загруженными, как правило, за счёт увеличения многозадачности. *Это ошибка мышления, основанного на локальной-оптимизации.*

Что на самом деле происходит, когда кто-то увеличивает уровень утилизации инструментов или людей в системах с вариативностью?

В Xerox есть дорогие большие цифровые печатные прессы в тестовой лаборатории. Часто возникает очередь разделяемого-ресурса из запросов на тестирование на одном из этих устройств. Без понимания того, как на самом деле работают очереди (точнее, вера в то, что они работают как на Рис. 1) подход к управлению должен был бы заключаться в том, чтобы способствовать резервированию и утилизации этих дорогостоящих систем, близкой к 100 процентам. Но реальность такова, что повсюду существует *вариативность*---стохастическая система. Тесты поступают случайно, некоторые  быстро завершаются с ошибкой, выполнение некоторых занимает целую вечность, иногда ломается оборудование и тому подобное. *Такая же вариативность существует и в поведении людей и в очередях работы, которую они выполняют* .

<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-5.JPG" alt="queueing-5.JPG">
  <figcaption>Тестовая лаборатория в  Xerox с очередями разделяемых-ресурсов для оборудования.</figcaption>
</figure>

### Моделирование Базовой Системы с Очередями и Единичным-Поступлением  

Как ведут себя эти системы---в дорожном трафике, тестовых лабораториях, или в традиционной разработке, где люди сталкиваются с очередями НЗР? У вас уже сложилось понимание этого из истории о трафике. Математически, это поведение может быть моделировано в виде вариаций систем М/М. М/М означает, что  that the внутренний порядок поступления в очередь является Марковским, а также  порядок обработки элементов очереди тоже *Марковский*. Что такое Марковский: Простая концепция---некий случайный (стохастический) процесс, в котором состояние в будущем не может быть детерминировано на основании его прошлых состояний (см. [Цепь Маркова](https://ru.wikipedia.org/wiki/Цепь_Маркова) прим. переводчика); то есть, похоже на “беспорядочную реальность.”

Наиболее распространённую, базовую модель очереди можно обозначить так: М/М/1/*∞* ---она имеет один обработчик (например, один тестовый принтер или команду) и бесконечную очередь. (Очереди в разработке, как правило, не бесконечны, но это упрощение не повлияет на базовый паттерн поведения системы.)

Теперь становится интереснее ... Как в системе М/М/1/*∞*, соотносится время цикла и время обслуживания с загрузкой обработчика---будь то тестовый принтер или люди, работающие с очередями НЗР? График "Ожидание в базовых системах М/М/1/∞." показывает это [[Smith07]](http://www.amazon.com/Flexible-Product-Development-Building-Changing/dp/0787995843/ref=sr_1_1?ie=UTF8&qid=1413673423&sr=8-1&keywords=flexible+product+development).

На графике "Ожидание в базовых системах М/М/1/∞." показаны *примерные* значения, поскольку факторы имеют случайную вариативность, например:

* запросы поступают в разное время с разной интенсивностью
* усилия, необходимые на тесты или программирование, занимают разное время
* люди могут работать быстрее или медленнее, заболеть, работать дольше или меньше


<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-6.png" alt="queueing-6.png">
  <figcaption>Ожидание в базовых системах М/М/1/∞.</figcaption>
</figure>


Существенный момент, который следует осознать, заключается в том, что элемент (например, такой, как запрос нового требования) начнёт попадать в очередь, ожидая обслуживания, задолго до того, как люди будут загружены на 100 процентов. Также удивительно увидеть влияние увеличения загрузки людей на время цикла: *Когда загрузка возрастает, в системе с большой вариативностью, среднее время цикла ухудшается, не улучшается.* Это контринтуитивно для заурядного бухгалтера или консультанта по вопросам управления  который был обучен “улучшать продуктивность путём увеличения утилизации ресурсов.” Большинство из них не были знакомы с теорией массового обслуживания have---как понимать стохастическую систему с очередями (людей, выполняющих работу с вариативностью)---и поэтому демонстрируют распространённую ошибку мышления.


Именно эта изменчивость реальной жизни, в основном, и увеличивает размер очереди и время ожидания в разработке продуктов.
{: .box_top_bottom .text_centered_bold }


### Моделирование Пакетной Системы с Очередями  (Традиционная Разработка)

Становится *ещё более* интересно (если вы сможете в это поверить)...  Базовая система М/М/1/*∞* подразумевает, что элементы (на тестирование, анализ, разработку, ...) прибывают изолированно, по *одному*---что прибывающие элементы никогда не группируются (не объединяются в пакеты). Однако в традиционной разработке продуктов, пакеты работ *в действительности* прибывают большими, объёмными партиями, такими как набор требований или задач на тестирование  или большой партии кода, которая должна быть интегрирована. Или ‘единственное’, как предполагается, требование может быть получено в форме: «обработка сделок с деривативами на рынке Бразилии», но что по факту тоже является пакетом под-требований. Этот последний момент очень важно отметить, поскольку большое требование никогда *не* проходит через систему как один пакет требований, хотя "со стороны" это и может ошибочно казаться одним цельным пакетом требований.


“Одно большое требование само по себе является пакетом” - это критически важный момент который будет рассмотрен далее.
{: .box_top_bottom .text_centered_bold }


Итак, вместо более простой системы с единичным-поступлением по модели М/М/1/∞ (на вход поступают отдельные, атомарные элементы работы), рассмотрим систему М[x]/М/1/∞ (на вход которой поступают пакеты элементов). Эта модель является лучшей аналогией традиционной продуктовой разработке.

<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-7.png" alt="queueing-7.png">
  <figcaption>Ожидание в  системе М[x]/М/1/∞ - нечто cхожее с традиционной  разработкой с пакетами переменного размера.</figcaption>
</figure>


С первого раза, люди могут не осознать такого поразительного и кажущегося нелогичным влияния на их время цикла.

Следующая история поможет разобраться: Представим человека или команду, загруженных на текущий момент на 50 процентов, вы обычно даёте им по одному требованию небольшого размера время от времени, которые поступают с некоторой случайностью и могут несколько отличаться в размере. Предположим  что им потребуется *две недели* фактической работы, чтобы выполнить некое требование-X. И предположим, что это простая система с одиночным поступлением. В таблице ниже приведена аппроксимация типовой ситуации:

<table style="width:100%">
  <tr>
    <th>Поступление</th>
    <th>Утилизация</th>
    <th>Время в Очереди (Ожидания)</th>
    <th>Время Обработки (Обслуживания)</th>
    <th>Время Цикла</th>
    <th>Коэффициент ВремяЦикла/ВремяОбработки</th>
  </tr>
  <tr>
    <td>единичное поступление</td>
    <td>50%</td>
    <td>2 недели</td>
    <td>2 недели</td>
    <td>4 недели</td>
    <td>2</td>
  </tr>
</table>
<br>
<br>


Далее, наоборот, предположим что вы обычно даёте на 50 процентов загруженной команде существено бóльшие *пакеты* требований или по ‘одному’ гигантскому требованию, которые фактически включают в себя большой пакет под-требований; такие пакеты поступают с некой случайностью во времени и могут отличаться по размеру. Предположим, что непосредственно на обслуживание некоего пакета-X или одиночного требования уходит *двадцать недель*.

Основываясь на предыдущей таблице, вот какие значения некоторые люди прогнозировали бы для ситуации поступлением больших партий:

<table style="width:100%">
  <tr>
    <th>Поступление</th>
    <th>Утилизация</th>
    <th>Время в Очереди</th>
    <th>Время Обработки</th>
    <th>Время Цикла</th>
    <th>Коэффициент ВремяЦикла/ВремяОбработки</th>
  </tr>
  <tr>
    <td>единичное поступление</td>
    <td>50%</td>
    <td>2 недели</td>
    <td>2 недели</td>
    <td>4 недели</td>
    <td>2</td>
  </tr>
  <tr>
    <td>а если большая партия?</td>
    <td>50%</td>
    <td>20 недель</td>
    <td>20 недель</td>
    <td>40 недель</td>
    <td>2</td>
  </tr>
</table>
<br>
<br>

Предсказание, основанное на внутреннем ощущении *линейно* увеличивает воздействие на время цикла. В среднем в десять раз возросло количество работы "пропущенной" через систему, значит в десять раз должно возрасти время цикла. Четыре недели вместо сорока недель.

Но так это не работает, потому что в систему введено больше вариативности. Что же в таком случае происходит на самом деле?

При 50-процентной утилизации, коэффициент времени цикла-к-обслуживанию  на самом деле ‘5’ в примере  М[x]/М/1/∞. Это приблизительно соответствует следующей ситуации, сильно отличающейся от предыдущего предположения:


<table style="width:100%">
  <tr>
    <th>Поступление</th>
    <th>Утилизация</th>
    <th>Время в Очереди</th>
    <th>Время Обработки</th>
    <th>Время Цикла</th>
    <th>Коэффициент ВремяЦикла/ВремяОбработки</th>
  </tr>
  <tr>
    <td>единичное поступление</td>
    <td>50%</td>
    <td>2 недели</td>
    <td>2 недели</td>
    <td>4 недели</td>
    <td>2</td>
  </tr>
  <tr>
    <td>большая партия</td>
    <td>50%</td>
    <td>80 недель</td>
    <td>20 недель</td>
    <td>100 недель</td>
    <td>5</td>
  </tr>
</table>
<br>
<br>



So pushing for high utilization rates of workers in this situation with big batches of work is a recipe for... sloooow. The reality will be a super-linear increase in cycle time. This impact on delay and queue size defies our instinct because people are not used to analyzing stochastic systems with queues. One might think, “If I make the work package ten times bigger, it will take ten times as long to exit the system.” Do not count on it.
Всё стало *намного* хуже. Конечно, это некие усреднения и они не могут быть приняты ни для какого реального случая и эта модель является упрощённой абстракцией реальной продуктовой разработки. Но вот почему понимание---и действие в соответствии с этим пониманием---теории массового обслуживания применимо для крупно-масштабной разработки. Потому что большие системы часто связаны с масштабными требованиям, и большим количеством работы (разработка требований, тестирование, интеграция, ...) в больших партиях, прибывающих в случайное время, с работниками, утилизация (загрузка) которых ожидается  около 100%  на протяжении всего времени. Это может оказать невероятное влияние не среднее время цикла.

И эти задержки ещё более усугубляются в традиционной последовательной разработке, поскольку  существуют *серии* процессов с очередями НЗР перед ними; это усугубляет вариативность и добавляет больше негативного влияния на общее среднее время цикла. *Закон Распределения Вариативности* [[HS08]](http://www.amazon.com/Factory-Physics-Wallace-J-Hopp/dp/1577667395/ref=sr_1_1?ie=UTF8&qid=1413686585&sr=8-1&keywords=Factory+Physics) показывает, что худшее место для вариативности (в плане негативного воздействия на время цикла) - в самом начале многоступенчатой системы с очередями. Это как раз то, что происходит в традиционной разработке на первой фазе анализа требований с большими порциями спецификаций.

### Заключение

Итак, что мы узнали?

* разработка продуктов  - это стохастическая система с очередями; она нелинейна и не детерминирована
* поведение стохастической системы с очередями противоречит нашим инстинктивным, внутренним ощущениям
* размер партии, размеры требования и степень утилизации влияют на размер очереди и время цикла нелинейным, случайным способом, который не является очевидным---другими словами, пропускная способность  может *уменьшаться*
* размер очереди влияет на время цикла
* в изменчивой системе, высокая утилизация *увеличивает* время цикла и снижает пропускную способность---традиционный подход к управлению ресурсами здесь не помогает [[например, McGrath04]](http://www.amazon.com/Next-Generation-Product-Development-Productivity/dp/0071435123) и может, наоборот, усугубить ситуацию если сосредоточиться  на локальной оптимизации загрузки рабочих вместо пропускной способности системы
* система с вариативностью и серией процессов с очередями НЗР еще больше усугубляет задержку; эта ситуация характеризует традиционную последовательную разработку ПО
* высокая вариативность на входе многоступенчатой системы с очередями имеет наиболее пагубные последствия








## Hidden Batches: Eyes for Batches

If you bake three cherry pies at the same time, then it is clear that there is a batch of three items. Things are not so clear in product development: What exactly is ‘one’ requirement? At one level, “handle bond derivative trades” is one requirement, but it is also a *composite requirement* or a batch of sub-requirements that can be split. These hidden batches need to be seen.

![queueing-8.jpg](/img/queueing_theory/queueing-8.jpg)

As we've seen, a system with queues behaving like a M[x]/M/1/∞ model and high utlization rates means that variable-sized big batches are bad for cycle time. And so-called *single* large items with variability are bad for cycle time, as they are hidden big batch. So, the implication for queue management in LeSS is this:

To reduce average cycle time, maintain slack in the system (not "100% busy") and (eventually, incrementally) reduce all apparently ‘single’ big items (requirements) in the Product Backlog to small and roughly equal-sized items.
{: .box_top_bottom .text_centered_bold }









## Hidden Queues: Eyes for Queues

When people join Toyota, they learn “Eyes for Waste.” They learn to see things as waste that they had not considered, such as *inventory* ---queues of stuff. Now, queues of *physical* things are easy for people to perceive, and to perceive as a problem... My goodness, there’s a *gigantic* pile of *Stuff* queuing up over there! Making any money from the pile? Are there defects in there? Does it need to be combined with other stuff before we can ship it? Do we need---and will we make money with---*each and every item* in the pile?

**Invisible queues**{: style="color: #1997C0"}---In traditional development there are also all kinds of queues, but because they are *invisible* they are not seen as queues or *keenly felt* as problems. If you are a business person who has invested ten million euros to create a gigantic pile of partially done *Stuff* sitting on the floor, not making any money, you walk by it and *see it* and you feel the pain and urgency to get it moving. And you think about no longer making big piles of partially done stuff. But product development people do not really see and feel the pain of their queues.

Yet, they *are* there. Queues of WIP---information, documents, and bits on a disk. Invisible queues. Product development people need a lesson in “Eyes for Queues” so that they can start to perceive what is going on, and develop a sense of urgency about reducing queue sizes.

**Visual management for tangible queues**{: style="color: #1997C0"}---To develop “eyes for queues” and a sense of attention, one lean practice is *visual management*, making *physical* tokens (not tokens in a computer program) for these queues. For example, a popular technique is to represent all the tasks for a Sprint on paper cards that are placed on the wall and moved around as tasks are completed. Similarly for the high-priority top of the Product Backlog. These *physical tangible* cards make truly visible the invisible queues that are hidden away inside computer. Hiding this information into computers defeats the purpose of lean visual (physical) management and the way *humans*---with countless eons of evolutionary instinct working with concrete things---need to *see and feel tangible queues* .


<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-9.JPG" alt="queueing-9.JPG">
  <figcaption>Visual (physical tokens) management make visible the invisible queues and WIP.</figcaption>
</figure>







## Indirect Benefits of Reducing Batch Size and Cycle Time

“Why bother? Our customers don’t want a release every two weeks, nor do they want just a sub-requirement.”

We get this question regularly from product groups and business people. They do not yet appreciate the advantages of small batches in short cycles:

* The *overall* larger release-cycle-time reduction that can come by eradicating queues and by applying queue management so that many development cycles are shorter.
* The elimination of **batch delay** , where one feature is unnecessarily held back because it is moving through the system attached to a larger batch of other requirements. Eliminating this provides another degree of freedom for the business to ship a smaller product earlier with the highest-priority features.
* And last but not least, there are *indirect* benefits due to the “*lake and rocks* ” effect described next.


### Indirect Benefits: The Lake and Rocks Metaphor

A metaphor shared in lean education: **the lake and rocks**. The depth of the water may represent the inventory level, batch size, iteration length, or cycle time. When the water is high (large batch or inventory size, or iteration length), many rocks are hidden. These rocks represent weaknesses. For example, consider an eighteen- month sequential release cycle with a massive batch transfer; inefficient testing, integration, and poor collaboration are all hidden below the surface of such a long cycle and such a large batch. But if we work with that group and ask, “Please deliver a small set of small features that is potentially shippable in two weeks, every two weeks,” then suddenly all the ineffective practices become painfully obvious.


<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-10.jpg" alt="queueing-10.jpg">
  <figcaption>Reducing batch size (or cycle time) exposes hidden weaknesses.</figcaption>
</figure>



Said another way, the *transaction cost* (overhead cost) of the old process cycle becomes unacceptable. That pain then becomes a force for improvement, because people cannot stand re-experiencing it each short cycle, and indeed it may simply be impossible to the iteration goals with the old inefficient system of development.

*Tip*: Not all ‘rocks’ are big or immediately visible. The lean journey---and the journey of Scrum---is to *start with the big rocks* that are most painfully obvious yet movable, and over time work on smaller impediments.

This causal loop diagram (the notation is explained in [Systems Thinking](./systems_thinking.html)) illustrates this lake and rocks effect in terms of a system dynamics model:

<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-11.png" alt="queueing-11.png">
  <figcaption>Indirect and delayed benefits of reducing batch and cycle size.</figcaption>
</figure>










## Applying Queue Management in LeSS

There are dozens of strategies to manage queues. [Managing the Design Factory](http://www.amazon.com/Managing-Design-Factory-Donald-Reinertsen/dp/0684839911/ref=sr_1_1?ie=UTF8&qid=1413688897&sr=8-1&keywords=managing+the+design+factory) by Don Reinertsen explains many. However, we want to focus on a few key steps in a LeSS context:

1. change the system to utterly eradicate queues
2. learn to see remaining queues with visual management
3. reduce variability
4. limit queue size

*Change the system*{: style="color: #1997C0"}---Must you manage *existing* queues? Step outside the box. For example,  feature teams and acceptance TDD and continuous deployment eliminate many queues in traditional development.

*Reduce variability*{: style="color: #1997C0"}---Some people first try to reduce work queues by increasing utilization or multitasking (with negative results), or by adding more developers. True, adding people---if they are talented---can help (there are exceptions), but it is expensive and takes time. People who have grasped queue management recognize a simpler place to start: Reduce [special-cause variability](https://en.wikipedia.org/wiki/Common_cause_and_special_cause_(statistics)), which includes reduction in batch size.

It is possible to view the Product Backlog as one big near-infinite priority queue, but we suggest a more fine-grained view. It has distinct subsets. One view is that it contains two subsets: (1) the list for the current release (which ideally is every Sprint), and (2) the “future backlog.” A second perspective is that the Product Backlog contains the following two subsets:

* the *clear-fine* subset of items that are clearly analyzed, well estimated, and fine grained enough to do in much less than Sprint for one team
* the *vague-coarse* subset of coarse-grained items needing more analysis, estimation, and splitting before entering the clear-fine subset

The "current release" and "future" subsets may both contain clear-fine and vague-coarse items. At the start of a release cycle, the "current release" typically contains mostly vague-coarse items, and Sprint by Sprint they are refined into clear-fine items, and then implemented.

This leads to some key points:

* It is common---and advisable---in LeSS to prioritize only the clear-fine subset of the "current release".
* In LeSS, this “clear-fine priority queue” is the critical queue of implementation work before the teams.
* The vague-coarse subset is a *feeding queue* of items into a Product Backlog Refinement process that adds high-quality small items to the clear-fine subset.

<figure>
  <img class="rounded shadowed" src="/img/queueing_theory/queueing-12.png" alt="queueing-12.png">
  <figcaption>Product Backlog contains several queues.</figcaption>
</figure>



Before getting carried away with the idea of variability reduction...new development is not manufacturing; without variation nothing *new* happens or is discovered. It is both appropriate and inevitable that there is variability in research and development. However, there are indeed varieties of variability than can be diminished---the topic of this section. In the terminology of Edwards Deming, there is **common-cause variation** and **special-cause variation** . The first category is *common noise* variation in the process, and not easy to assign a specific cause. On the other hand, special-cause variation---also known as **assignable variation**---can be identified. For example, *variation in feature-request size* is special-cause variation. And *working on poorly analyzed unclear requirements* is special-cause variation. By reducing identifiable special-cause variation---in Less or work processes---a system with queues has improved average throughput.

*Variability* is one of the three sources of waste in lean thinking (the other two are *overburden* and *non-value-add actions* ). With an understanding of queueing theory, it may be clearer why variability is considered a source of waste.

What are some sources or kinds of variability in LeSS?

* big batches and big items
* ambiguity of what items mean
* ambiguity of how to design/implement items
* different (estimated) efforts for different items
* number of items in the "current release" clear-fine priority queue
* estimate-versus-actual effort variance, which can reflect what/how ambiguity, unskillful estimation, learning, and much more
* the arrival rate of items into the clear-fine priority queue of the current release
* team and individual variability
* overloading or failure of shared resources, such as a testing lab
{: .two_columns .box_top_bottom }

... and more. In queueing-model terminology, they usually boil down to variability in the *service* and *arrival rate* .

In lean thinking, *flow* is a key principle---and flow requires reduction or elimination of variability. That is why *leveling* is also a lean principle; it is an antidote to variability and helps move toward flow.



### Reducing Special-Cause Variability in LeSS

This leads to some special-cause-variability-reduction suggestions in LeSS:

**Reduce variability by a small queue (buffer) of clear-fine, similar-sized user stories in the Release Backlog**{: style="color: #1997C0"}---In Toyota, a small buffer of high-quality inventory is used to smooth or level the introduction of work to a downstream process. This inventory (a temporarily necessary waste) positively supports *level pull* because the LeSS feature teams now have a queue of similar-sized items to work on; no waiting and fewer surprises. Item in the vague-coarse subset of the current release have high *what/how ambiguity* and are large; so choosing those for implementation is unskillful because it increases variability.

**Reduce variability by holding a Product Backlog Refinement (PBR) workshop each Sprint**{: style="color: #1997C0"}--In LeSS, each Sprint must include PBR to prepare items so that they are ready for future Sprints. This reduces what/how ambiguity or variability, plus reduces estimation variability because re-estimation may improve as people learn. During this activity, split items *into small and roughly equally items*. This reduces batch size and its attendant variability. It also reduces *batch delay*---the artificial holding back of an important sub-feature because it was stuck to a larger batch of features, some of which are less important. These repeating workshops also creates a *regular cadence* of adding clear-fine items to the queue, reducing variability in arrival rate.

**Reduce variability by stable feature teams**{: style="color: #1997C0"}---Use stable long-lived feature teams in LeSS to reduce the variability in the ‘servers’---the teams. Also, the cross-functional, cross-component feature teams increase parallelism and flow because an item can flow to any one of several available teams.

**Reduce variability by timeboxed effort-boxed learning goals**{: style="color: #1997C0"}---This tip is most useful in domains with large *research-oriented* requirements. It reduces *what/how ambiguity* and variability. Sometimes non-trivial investigation is needed just to *start* to understand a feature. For example, we were once consulting at a site in Budapest; the mobile-telecom product group wanted to provide “push to talk over cellular.” The international standards document for this is *thousands* of pages. Just to vaguely grasp the topic is a formidable effort. One approach is to ask a team to “study the subject.” Yet, that is fuzzy unbounded work that will introduce more service variability and may expand into the lean waste of over-processing. An alternate approach---a suggestion in LeSS---is to offer a team a timeboxed and effort-boxed goal to learn. Perhaps the concrete goal is to present a research report at the end of the Sprint, in addition to their work implementing items. For example, “*introductory report on push to talk, maximum 30 person hours* .” A leveled amount of effort is put into the research and learning, balanced with the team also implementing items. The Product Owner may then decide to invest more bounded effort into another cycle of research in a future Sprint (probably more focused as the subject clarifies), until finally the subject is clear enough for people to start writing, splitting, and estimating items.


### Reducing Queue Sizes in LeSS

Another queue-management technique is to limit queue size. This does not necessarily reduce variability, but it has other virtues. In a traditional development first-in first-out (FIFO) WIP queue, a *long* queue is a problem because it will take forever for an item to move forward through the queue and eventually be finished---a straightforward reason to limit FIFO WIP queue size.

That problem is less pernicious in a Product Backlog *priority queue*, since it can be re-sorted---something just added can move to the head of the list. Still, there are good reasons to limit the number of items in the clear-fine priority queue of the current release:

* A long list of fine-grained complex features is hard to understand and prioritize. In large-product development, we regularly hear complaints from the Product Owner that the backlog is too big for them to “get their head around.”
* A big backlog of clearly analyzed, finely split and well-estimated items has usually had some investment in it, proportional to its size. It is WIP with no return on that investment. As always with WIP or inventory, that is a financial risk.
* People forget details over time. All the items in the clear-fine subset have been through in-depth analysis in Product Backlog Refinement workshops. If that list is short, there is a good chance that the team implementing the item has *recently* analyzed it in a workshop, perhaps within the last two months. If the queue is very long and growing, there is a greater chance the team will take on an item analyzed long ago. Even though there will probably be written documentation that was generated long ago in the workshop, it will of course be imperfect, and their grasp of the details will be hazy and stale.

## Conclusion

Queue management can become a *hammer* so that you go looking for queue *nails* . Resist the temptation to manage existing queues---that is an inside-the-box response to the problem. Rather, consider doing *system kaizen* so that the underlying system is changed in some way so that queues can no longer form or exist. Parallelizing with cross-functional teams and acceptance test-driven development are common examples, but there are more. Only apply queue management---a *point kaizen* tactic---when you cannot eradicate a queue.

## Recommended Readings

There are dozens, if not hundreds, of general texts on queueing theory. More specifically, we suggest readings that make the connection between this subject and product development:

* [Managing the Design Factory](http://www.amazon.com/Managing-Design-Factory-Donald-Reinertsen/dp/0684839911/ref=sr_1_1?ie=UTF8&qid=1413691933&sr=8-1&keywords=managing+the+design+factory) by Don Reinertsen is a classic introduction on queueing theory and product development.

* [Flexible Product Development](http://www.amazon.com/Flexible-Product-Development-Building-Changing/dp/0787995843/ref=sr_1_1?ie=UTF8&qid=1413691975&sr=8-1&keywords=flexible+product+development) by Preston Smith was the first widely-popular general product development book that introduced agile software development concepts---including Scrum and Extreme Programming---to a broader audience. This text includes an analysis of queueing theory and variability, and their relationship to development.
